import streamlit as st
import pandas as pd
import requests
from collections import Counter
from datetime import datetime
from xml.etree import ElementTree
import re

# -----------------------------
# 1ï¸âƒ£ é¡µé¢é…ç½®
# -----------------------------
st.set_page_config(page_title="Nova 2026 ç©¿é€çœ‹æ¿", page_icon="ğŸ›¡ï¸", layout="wide")
st.title("ğŸ›¡ï¸ æŠ•è¡Œçº§ 7D å…¨ç½‘ç©¿é€ç³»ç»Ÿ")
st.caption(f"ç³»ç»Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ¨¡å¼: è°·æ­Œå…¨ç½‘æ¢æµ‹ + 2026çƒ­è¯åº“")

# -----------------------------
# 2ï¸âƒ£ 2026 æ ¸å¿ƒæ•°æ®å­—å…¸ (çƒ­è¯æ¿å—å¢åŠ )
# -----------------------------
SECTOR_CONFIG = {
    "åŒ»è¯/ç”Ÿç‰©": {"keywords": "(GLP-1 OR ADCè¯ç‰© OR åˆ›æ–°è¯é—®è¯¢ OR å‡ºæµ·æˆæƒ OR åŒ»ç–—åˆè§„)", "stocks": ["600276", "300760", "603259"]},
    "AIç®—åŠ›/å°è£…": {"keywords": "(ç»ç’ƒåŸºæ¿ OR HBM4 OR ç®—åŠ›ç§Ÿèµ OR ç¡…å…‰æ¨¡å— OR å…ˆè¿›å°è£…)", "stocks": ["603501", "688012", "002415"]},
    "äººå½¢æœºå™¨äºº": {"keywords": "(è¡Œæ˜Ÿä¸æ  OR çµå·§æ‰‹ OR å‡é€Ÿå™¨ OR è§¦è§‰ä¼ æ„Ÿå™¨ OR ç‰¹æ–¯æ‹‰æœºå™¨äºº)", "stocks": ["603728", "300024", "002031"]},
    "å•†ä¸šèˆªå¤©/ä½ç©º": {"keywords": "(eVTOL OR åƒå¸†æ˜Ÿåº§ OR ä½ç©ºç©ºåŸŸ OR å«æ˜Ÿäº’è”ç½‘ OR é£è¡Œæ±½è½¦)", "stocks": ["002085", "600118", "300455"]},
    "æ–°èƒ½æº/å‚¨èƒ½": {"keywords": "(å…¨å›ºæ€ç”µæ±  OR é’ ç¦»å­ç”µæ±  OR æ„ç½‘å‹å‚¨èƒ½ OR é’™é’›çŸ¿)", "stocks": ["300750", "002594", "300274"]},
    "ç§‘åˆ›é‡ç»„/èµ„æœ¬": {"keywords": "(ç§‘åˆ›æ¿å…«æ¡ OR å¹¶è´­é‡ç»„ OR å€Ÿå£³ OR èµ„äº§æ³¨å…¥ OR ä¸¾ç‰Œ)", "stocks": ["600104", "000157", "600606"]},
    "åœ°äº§/å®è§‚": {"keywords": "(æˆ¿åœ°äº§æ”¶å‚¨ OR å­˜é‡æˆ¿è´· OR æˆ¿åœ°è”åŠ¨ OR é™æ¯)", "stocks": ["600048", "000002", "601155"]}
}

# -----------------------------
# 3ï¸âƒ£ ä¸é™æµä¸ªè‚¡è¡Œæƒ… (è…¾è®¯æºæ›´ç¨³)
# -----------------------------
@st.cache_data(ttl=60)
def get_realtime_stocks(sector_name):
    stock_ids = SECTOR_CONFIG.get(sector_name, {}).get("stocks", ["600519"])
    formatted_ids = ",".join([f"sh{s}" if s.startswith('6') else f"sz{s}" for s in stock_ids])
    url = f"http://qt.gtimg.cn/q={formatted_ids}"
    try:
        res = requests.get(url, timeout=5).text
        data = []
        for line in res.splitlines():
            if '~' in line:
                p = line.split('~')
                data.append({"åç§°": p[1], "æœ€æ–°ä»·": p[3], "æ¶¨è·Œå¹…": f"{p[32]}%"})
        return pd.DataFrame(data)
    except:
        return pd.DataFrame()

# -----------------------------
# 4ï¸âƒ£ æ ¸å¿ƒæŠ“å–å¼•æ“ (è°·æ­Œ 7D åŠŸèƒ½ä¿ç•™)
# -----------------------------
@st.cache_data(ttl=600)
def fetch_news_via_google(query=""):
    try:
        # å®½å£å¾„æœç´¢ï¼šé”å®š 7 å¤©å†…ï¼ŒåŒ…å«ä¸»æµè´¢ç»æº
        search_query = f"({query}) (site:cls.cn OR site:qq.com OR site:163.com OR site:sina.com.cn OR site:jiemian.com) when:7d"
        url = f"https://news.google.com/rss/search?q={search_query}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=10)
        root = ElementTree.fromstring(res.content)
        
        records = []
        for item in root.findall('.//item')[:25]:
            full_title = item.find('title').text
            title = full_title.rsplit(' - ', 1)[0] if ' - ' in full_title else full_title
            source = full_title.rsplit(' - ', 1)[1] if ' - ' in full_title else "å…¨ç½‘"
            
            records.append({
                "source": source,
                "title": title,
                "time": item.find('pubDate').text[5:16],
                "link": item.find('link').text
            })
        return pd.DataFrame(records)
    except:
        return pd.DataFrame()

# -----------------------------
# 5ï¸âƒ£ çƒ­è¯ç»Ÿè®¡å‡½æ•°
# -----------------------------
def analyze_hot_keywords(df):
    if df.empty: return []
    # ç®€å•çš„åˆ†è¯æ¸…æ´—é€»è¾‘ï¼ˆå»é™¤æ— æ„ä¹‰è¯ï¼‰
    stop_words = ["è´¢ç»", "æ–°é—»", "å‘å¸ƒ", "å…¬å¸", "è¿›è¡Œ", "åˆ†æ", "å…³æ³¨", "è¿›è¡Œ", "ä¸­å›½"]
    text = " ".join(df['title'].tolist())
    words = re.findall(r'\w{2,}', text) # æå–2å­—ä»¥ä¸Šè¯
    filtered_words = [w for w in words if w not in stop_words and not w.isdigit()]
    return Counter(filtered_words).most_common(8)

# =========================
# 6ï¸âƒ£ Streamlit UI äº¤äº’
# =========================

st.sidebar.header("ğŸ” å®¡è®¡æœç´¢æ§åˆ¶å°")
manual_key = st.sidebar.text_input("æ³¨å…¥æ‰‹åŠ¨å…³é”®è¯", placeholder="å¦‚ï¼šå¸‚å€¼ç®¡ç† / å›ºæ€ç”µæ± ")
probe_trigger = st.sidebar.button("ğŸš€ æ‰§è¡Œç©¿é€æ¢æµ‹", use_container_width=True)
st.sidebar.divider()

if probe_trigger and manual_key:
    st.subheader(f"ğŸš€ ä¸“é¡¹æ¢æµ‹ï¼š{manual_key}")
    news = fetch_news_google(manual_key)
    if not news.empty:
        # çƒ­è¯åˆ†æ
        hot_tags = analyze_hot_keywords(news)
        st.write("ğŸ·ï¸ **åŠ¨æ€çƒ­è¯ç»Ÿè®¡ï¼š** " + " ".join([f"`{w[0]}({w[1]})`" for w in hot_tags]))
        st.dataframe(news, use_container_width=True, hide_index=True)
    else:
        st.warning("æœªèƒ½å‘ç°ç›¸å…³æƒ…æŠ¥ã€‚")

else:
    # 1. æ¿å—çœ‹æ¿æ¨¡å¼
    st.subheader("ğŸ­ è¡Œä¸šæ·±åº¦ç©¿é€")
    selected_sector = st.selectbox("é€‰æ‹©å®¡è®¡æ¿å—", list(SECTOR_CONFIG.keys()))

    col1, col2 = st.columns([1, 2.5])

    with col1:
        st.write(f"ğŸ“Š **{selected_sector}** å®æ—¶æ ‡çš„ï¼š")
        stock_df = get_realtime_stocks(selected_sector)
        if not stock_df.empty:
            st.table(stock_df)
            
        # ç»Ÿè®¡åˆ†æå±•ç¤ºåœ¨è¡Œæƒ…ä¸‹æ–¹
        st.divider()
        st.write("ğŸ“ˆ **æœ¬å‘¨èˆ†æƒ…çƒ­ç‚¹åˆ†å¸ƒ**")
        q_words = SECTOR_CONFIG[selected_sector]["keywords"]
        sector_news = fetch_news_via_google(q_words)
        hot_tags = analyze_hot_keywords(sector_news)
        for tag, count in hot_tags:
            st.write(f"Â· {tag}")
            st.progress(min(count * 10, 100))

    with col2:
        st.write(f"ğŸ“° **{selected_sector}** 7D å…³é”®åŠ¨æ€ï¼š")
        if not sector_news.empty:
            for _, row in sector_news.iterrows():
                with st.container():
                    c_a, c_b = st.columns([5, 1])
                    c_a.markdown(f"**[{row['source']}]** {row['title']}")
                    c_a.caption(f"â³ {row['time']}")
                    c_b.link_button("ç©¿é€", row['link'], use_container_width=True)
                    st.divider()
        else:
            st.info("å¹¶æœªå‘ç°ç¬æ—¶åŠ¨æ€ï¼Œç³»ç»Ÿæ­£åœ¨é‡è¯•é”šç‚¹ç©¿é€...")

st.divider()
# 2. å…¨é‡æµ
st.subheader("ğŸ”¥ å¸‚åœºå…¨å±€å¼‚åŠ¨æµ (7Då›æº¯)")
main_news = fetch_news_via_google("å¹¶è´­é‡ç»„ OR è‚¡æƒè½¬è®© OR å¼‚åŠ¨ OR ä¸¾ç‰Œ")
if not main_news.empty:
    st.dataframe(main_news[['time', 'source', 'title']], use_container_width=True, hide_index=True)

st.markdown("---")
st.caption("Nova å®¡è®¡è„šæ³¨ï¼šé‡‡ç”¨ 7D å…¨ç½‘é•œåƒè”åŠ¨é€»è¾‘ã€‚çƒ­è¯ç»Ÿè®¡é€šè¿‡å®æ—¶ NLP è¯­ä¹‰æå–ç”Ÿæˆã€‚")
