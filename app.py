import streamlit as st
import pandas as pd
import requests
from collections import Counter
from datetime import datetime
import re
from xml.etree import ElementTree

# -----------------------------
# 1ï¸âƒ£ é¡µé¢é…ç½®
# -----------------------------
st.set_page_config(page_title="Nova æŠ•è¡Œçº§æ–°é—»çœ‹æ¿ (ç©¿é€ç‰ˆ)", page_icon="ğŸ›¡ï¸", layout="wide")
st.title("ğŸ›¡ï¸ æŠ•è¡Œçº§æ–°é—»æ¿å—ç©¿é€ç³»ç»Ÿ")
st.caption(f"ç³»ç»Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ•°æ®æµçŠ¶æ€: ç©¿é€éš”ç¦»æ¨¡å¼")

# -----------------------------
# 2ï¸âƒ£ æ ¸å¿ƒæ•°æ®æŠ“å– (RSS é•œåƒæµ)
# -----------------------------
@st.cache_data(ttl=600)
def fetch_news_stable():
    try:
        # ä½¿ç”¨ Google News èšåˆï¼Œç¡®ä¿ 2026 å¹´äº‘ç«¯éƒ¨ç½²ä¸è¢«è´¢è”ç¤¾ WAF å°é”
        url = "https://news.google.com/rss/search?q=è´¢è”ç¤¾+å¹¶è´­+å›è´­+IPO+æ¿å—+å¼‚åŠ¨&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=10)
        root = ElementTree.fromstring(res.content)
        records = []
        for item in root.findall('.//item')[:50]:
            records.append({
                "title": item.find('title').text,
                "time": item.find('pubDate').text,
                "link": item.find('link').text
            })
        return pd.DataFrame(records)
    except:
        return pd.DataFrame()

# -----------------------------
# 3ï¸âƒ£ æ¿å—ä»£ç åº“ä¸å…³è”è¯ç°‡ (æ ¸å¿ƒæ‰©å……)
# -----------------------------
SECTOR_CONFIG = {
    "æ–°èƒ½æº": {"code": "BK0998", "keywords": ["é”‚ç”µ", "ç”µæ± ", "å®å¾·", "å‚¨èƒ½", "ç”µç½‘", "å…‰ä¼"]},
    "åŒ–å·¥": {"code": "BK0436", "keywords": ["æ¶¨ä»·", "ææ–™", "ç£·", "æ°Ÿ", "äº§èƒ½", "ç‚¼åŒ–"]},
    "åŸææ–™": {"code": "BK0486", "keywords": ["æ°´æ³¥", "å»ºæ", "é’¢é“", "çŸ¿äº§", "é‡‘å±"]},
    "åŒ»è¯": {"code": "BK0506", "keywords": ["ç”Ÿç‰©", "åˆ›æ–°è¯", "é›†é‡‡", "ä¸´åºŠ", "ç–«è‹—"]},
    "ç»¼åˆ/é‡ç»„": {"code": "BK0110", "keywords": ["å¹¶è´­", "é‡ç»„", "è‚¡æƒ", "å£³èµ„æº", "èµ„äº§"]},
    "å…‰ä¼": {"code": "BK0933", "keywords": ["ç»„ä»¶", "ç¡…ç‰‡", "éš†åŸº", "é€†å˜å™¨", "å¤šæ™¶ç¡…"]},
    "AI": {"code": "BK1096", "keywords": ["å¤§æ¨¡å‹", "ç®—åŠ›", "èŠ¯ç‰‡", "è‹±ä¼Ÿè¾¾", "æ™ºç®—"]},
    "å…ƒå®‡å®™": {"code": "BK1009", "keywords": ["è™šæ‹Ÿç°å®", "VR", "AR", "æ•°å­—äºº", "æ²‰æµ¸"]},
    "ä½ç©ºç»æµ": {"code": "BK1158", "keywords": ["æ— äººæœº", "é£è¡Œæ±½è½¦", "eVTOL", "ç©ºç®¡"]},
    "ç§‘æŠ€": {"code": "BK0707", "keywords": ["åŠå¯¼ä½“", "é›†æˆç”µè·¯", "å°æµ‹", "å…‰åˆ»æœº"]},
    "åœ°äº§": {"code": "BK0451", "keywords": ["å­˜é‡æˆ¿", "æˆ¿è´·", "åœŸæ‹", "æ”¶å‚¨", "ä¿éšœæˆ¿"]}
}

@st.cache_data(ttl=3600)
def get_sector_stocks():
    sector_data = {}
    for name, config in SECTOR_CONFIG.items():
        try:
            url = f"http://push2.eastmoney.com/api/qt/clist/get?pn=1&pz=15&po=1&np=1&fltt=2&invt=2&fs=b:{config['code']}&fields=f12,f14"
            res = requests.get(url, timeout=5).json()
            stocks = [f"{item['f14']}({item['f12']})" for item in res.get('data', {}).get('diff', [])]
            sector_data[name] = stocks
        except:
            sector_data[name] = []
    return sector_data

# -----------------------------
# 4ï¸âƒ£ ç©¿é€æ˜ å°„é€»è¾‘
# -----------------------------
def filter_news_by_sector(news_df, sector_name):
    if news_df.empty: return news_df
    
    # è·å–è¯¥æ¿å—çš„å…³è”ç‰¹å¾è¯
    config = SECTOR_CONFIG.get(sector_name, {})
    keywords = [sector_name] + config.get("keywords", [])
    
    # æŠ•è¡Œé€šç”¨é«˜æƒè¯
    keywords += ["å›è´­", "å¢æŒ", "å¹¶è´­", "å¼‚åŠ¨"]
    
    # æ„å»ºæ­£åˆ™åŒ¹é…æ¨¡å¼
    pattern = "|".join(keywords)
    return news_df[news_df['title'].str.contains(pattern, case=False, na=False)]

# =========================
# 5ï¸âƒ£ Streamlit UI
# =========================
st.sidebar.header("ğŸ” å®¡è®¡å¹²é¢„")
manual_key = st.sidebar.text_input("æ‰‹åŠ¨å…³é”®è¯æœç´¢", placeholder="å¦‚ï¼šå¸‚å€¼ç®¡ç†")

news_df = fetch_news_stable()
sector_map = get_sector_stocks()

if not news_df.empty:
    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¿å—æ·±åº¦ç©¿é€ ---
    st.subheader("ğŸ­ æ¿å—æ·±åº¦ç©¿é€")
    selected_sector = st.selectbox("é€‰æ‹©å®¡è®¡æ¿å—", list(SECTOR_CONFIG.keys()))
    
    c1, c2 = st.columns([1, 2])
    with c1:
        st.write(f"ğŸ“Œ **{selected_sector}** æ ¸å¿ƒæˆåˆ†è‚¡ï¼š")
        stocks = sector_map.get(selected_sector, [])
        if stocks:
            st.code("\n".join(stocks), language="text")
        else:
            st.warning("è¡Œæƒ…æ¥å£é™æµä¸­")

    with c2:
        # æ ¸å¿ƒæ”¹è¿›ï¼šæ ¹æ®é€‰ä¸­çš„æ¿å—ï¼Œè‡ªåŠ¨ç©¿é€ç›¸å…³æ–°é—»
        st.write(f"ğŸ“° **{selected_sector}** æ¿å—å…³è”æ–°é—»ï¼š")
        sector_related_news = filter_news_by_sector(news_df, selected_sector)
        
        if not sector_related_news.empty:
            for _, row in sector_related_news.head(8).iterrows():
                with st.expander(f"{row['title']}"):
                    st.caption(f"å‘å¸ƒæ—¶é—´: {row['time']}")
                    st.markdown(f"[åŸæ–‡é“¾æ¥]({row['link']})")
        else:
            st.info(f"å½“å‰æµä¸­æš‚æ— ä¸ {selected_sector} å¼ºç›¸å…³çš„çº¿ç´¢")

    st.divider()

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šå…¨é‡å®¡è®¡æµ ---
    st.subheader("ğŸ” å…¨é‡æ–°é—»å®¡è®¡æµ")
    search_term = manual_key if manual_key else ""
    display_news = news_df[news_df['title'].str.contains(search_term)] if search_term else news_df
    
    for _, row in display_news.head(15).iterrows():
        with st.expander(f"{row['title']}"):
            st.write(f"å‘å¸ƒæ—¶é—´: {row['time']}")
            st.markdown(f"[è·³è½¬åŸæ–‡]({row['link']})")
            # æ ‡è®°è¯¥æ–°é—»å‘½ä¸­äº†å“ªäº›æ¿å—
            hits = [name for name, cfg in SECTOR_CONFIG.items() if any(k in row['title'] for k in [name]+cfg['keywords'])]
            if hits:
                st.info(f"å®¡è®¡æ ‡è®° - å…³è”æ¿å—: {', '.join(hits)}")

else:
    st.error("æ— æ³•å»ºç«‹å®‰å…¨è¿æ¥ã€‚Novaï¼Œè¯·æ£€æŸ¥æœ¬åœ°ä»£ç†æˆ–äº‘ç«¯é˜²ç«å¢™è®¾ç½®ã€‚")

st.markdown("---")
st.caption("Nova å®¡è®¡é€»è¾‘ï¼šç¬¬ä¸€é€šè¿‡è¯ç°‡æ¨¡ç³Šæ˜ å°„ï¼Œæ¬¡ä¹‹ä¸‹é’»æˆåˆ†è‚¡ï¼Œç»ˆäºå…¨çƒ RSS éš”ç¦»æŠ“å–ã€‚")
