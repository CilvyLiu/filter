import streamlit as st
import pandas as pd
import requests
from xml.etree import ElementTree
from collections import Counter
from datetime import datetime
import akshare as ak

# ------------------------
# 1ï¸âƒ£ é¡µé¢é…ç½®
# ------------------------
st.set_page_config(
    page_title="Nova Aè‚¡æŠ•è¡Œ+è¡Œä¸šå†³ç­–çœ‹æ¿",
    page_icon="ğŸ›¡ï¸",
    layout="wide"
)

# ------------------------
# 2ï¸âƒ£ é…ç½®å­—å…¸ (ç”± Nova æä¾›å¹¶ä¼˜åŒ–å¯¹é½)
# ------------------------
KEY_WEIGHTS = {
    # æŠ•è¡ŒåŠæ”¿ç­–ç±»
    'å›è´­': 5, 'å¢æŒ': 5, 'å¹¶è´­': 4, 'IPO': 4, 'é™å”®è§£ç¦': 4, 'åˆ†çº¢': 3, 'é™å‡†': 3, 'æ³¨å†Œåˆ¶': 3, 'èèµ„èåˆ¸': 2,
    # è¡Œä¸šåŠæ¿å—ç±»
    'åŒ–å·¥': 4, 'åŸææ–™': 4, 'æ–°èƒ½æº': 4, 'åŒ»è¯': 4, 'ç§‘æŠ€': 4, 'åœ°äº§': 3, 'èƒ½æº': 4, 'é’¢é“': 3, 'ç”µæ± ': 3, 'å…‰ä¼': 3
}

# æ˜ å°„å­—å…¸ï¼šç¡®ä¿ä¸ AkShare/åŒèŠ±é¡ºæ ‡å‡†åå¯¹é½
KEYWORD_TO_SECTOR = {
    'æ–°èƒ½æº': 'ç”µåŠ›è®¾å¤‡',
    'åŒ–å·¥': 'åŸºç¡€åŒ–å·¥',
    'åŸææ–™': 'å»ºç­‘ææ–™',
    'åŒ»è¯': 'åŒ»è¯ç”Ÿç‰©',
    'ç§‘æŠ€': 'åŠå¯¼ä½“åŠå…ƒä»¶',
    'åœ°äº§': 'æˆ¿åœ°äº§å¼€å‘',
    'èƒ½æº': 'çŸ³æ²¹åŠ å·¥è´¸æ˜“',
    'é’¢é“': 'é’¢é“',
    'ç”µæ± ': 'ç”µåŠ›è®¾å¤‡',
    'å…‰ä¼': 'å…‰ä¼è®¾å¤‡',
    'å›è´­': 'ä¸­å­—å¤´',
    'å¢æŒ': 'æ²ªæ·±300',
    'å¹¶è´­': 'èµ„äº§é‡ç»„',
    'IPO': 'æ¬¡æ–°è‚¡'
}

# ------------------------
# 3ï¸âƒ£ æ•°æ®æŠ“å– (ä¸å° IP ç‰ˆ)
# ------------------------
@st.cache_data(ttl=600)
def fetch_ib_news(limit=35):
    """é€šè¿‡ RSS æŠ“å–æŠ•è¡ŒåŠè¡Œä¸šæ”¿ç­–æ–°é—»"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        # æœç´¢ç»„åˆï¼šåˆå¹¶äº†ä½ æä¾›çš„æ‰€æœ‰æ ¸å¿ƒçƒ­è¯
        search_query = "æŠ•èµ„é“¶è¡Œ+å¹¶è´­+å›è´­+IPO+åŒ–å·¥+åŸææ–™+æ–°èƒ½æº+åŒ»è¯"
        url = f"https://news.google.com/rss/search?q={search_query}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        res = requests.get(url, headers=headers, timeout=10)
        root = ElementTree.fromstring(res.content)
        records = []
        for item in root.findall('.//item')[:limit]:
            records.append({
                "title": item.find('title').text,
                "link": item.find('link').text,
                "time": item.find('pubDate').text,
                "content": item.find('title').text
            })
        return pd.DataFrame(records)
    except Exception as e:
        return pd.DataFrame()

# ------------------------
# 4ï¸âƒ£ æ ¸å¿ƒå¤„ç†é€»è¾‘
# ------------------------
def calculate_hotwords(df, manual_key=None):
    """åº”ç”¨ Nova ä¸“å®¶æƒé‡å­—å…¸"""
    current_weights = KEY_WEIGHTS.copy()
    if manual_key:
        current_weights[manual_key] = 10  # æ‰‹åŠ¨å¹²é¢„è®¾ä¸ºæœ€é«˜æƒé‡

    counter = Counter()
    for text in df['content']:
        for k, w in current_weights.items():
            if k in str(text):
                counter[k] += w
    
    res_df = pd.DataFrame(counter.most_common(20), columns=["word", "æƒé‡åˆ†"])
    res_df['æ¿å—'] = res_df['word'].apply(lambda x: KEYWORD_TO_SECTOR.get(x, 'å…¶ä»–/ç»¼åˆ'))
    return res_df

@st.cache_data(ttl=1800)
def get_sector_stocks(sector_name):
    """ä¸‹é’»è·å– A è‚¡æˆåˆ†è‚¡è¡Œæƒ…"""
    if sector_name in ['å…¶ä»–/ç»¼åˆ', 'ç»¼åˆ']: return pd.DataFrame()
    try:
        # è·¯å¾„ A: åŒèŠ±é¡ºè¡Œä¸šæˆåˆ† (æœ¬åœ°è¿è¡ŒæˆåŠŸç‡æé«˜)
        df = ak.stock_board_cons_ths(symbol=sector_name)
        return df[['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…']].head(20)
    except:
        return pd.DataFrame()

# ------------------------
# 5ï¸âƒ£ UI æ¸²æŸ“å±‚
# ------------------------
st.title("ğŸ›¡ï¸ Nova Aè‚¡æŠ•è¡Œ+è¡Œä¸šå†³ç­–çœ‹æ¿")
st.caption(f"ç³»ç»Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ•°æ®æµæ¨¡å¼: RSS ç©¿é€éš”ç¦»")

# 7.1 ä¾§è¾¹æ 
st.sidebar.header("ğŸ” å®¡è®¡å¹²é¢„")
manual_key = st.sidebar.text_input("æ³¨å…¥ä¸´æ—¶å…³é”®è¯", placeholder="å¦‚ï¼šå¸‚å€¼ç®¡ç†")

# è·å–æ•°æ®
news_df = fetch_ib_news()

if not news_df.empty:
    # 7.2 çƒ­è¯æ’è¡Œæ¦œ
    st.subheader("ğŸ”¥ å®æ—¶çƒ­åº¦æƒé‡æ¦œ")
    hotwords_df = calculate_hotwords(news_df, manual_key)
    st.dataframe(hotwords_df, use_container_width=True, hide_index=True)

    # å¸ƒå±€å±•ç¤º
    col1, col2 = st.columns([2, 3])

    with col1:
        st.subheader("ğŸ“° æœ€æ–°æ”¿ç­–å¿«è®¯")
        for _, row in news_df.head(10).iterrows():
            with st.expander(f"{row['title']}", expanded=False):
                st.write(f"å‘å¸ƒæ—¶é—´: {row['time']}")
                st.markdown(f"[åŸæ–‡é“¾æ¥]({row['link']})")

    with col2:
        st.subheader("ğŸ­ è¡Œä¸šç©¿é€è¡Œæƒ…")
        # è”åŠ¨ï¼šæ ¹æ®çƒ­åº¦æ¦œé€‰æ‹©æ¿å—
        sector_options = [s for s in hotwords_df['æ¿å—'].unique() if s != 'å…¶ä»–/ç»¼åˆ']
        if sector_options:
            selected_sector = st.selectbox("é€‰æ‹©çƒ­ç‚¹æ¿å—æŸ¥çœ‹æˆåˆ†è‚¡", sector_options)
            if selected_sector:
                stocks = get_sector_stocks(selected_sector)
                if not stocks.empty:
                    st.success(f"å·²é”å®š {selected_sector} æ ¸å¿ƒæˆåˆ†è‚¡è¡Œæƒ…")
                    st.dataframe(stocks, use_container_width=True, hide_index=True)
                else:
                    st.info(f"äº‘ç«¯æ¥å£æš‚æ— æ³•ç›´æ¥ç©¿é€ {selected_sector} è¡Œæƒ…ï¼Œå»ºè®®æœ¬åœ°è¿è¡Œã€‚")
        else:
            st.info("å½“å‰çƒ­è¯æš‚æ— ç‰¹å®šè¡Œä¸šæ˜ å°„ï¼Œè¯·æŸ¥çœ‹å®è§‚å¿«è®¯ã€‚")

    # 7.3 æœç´¢åŠŸèƒ½
    st.divider()
    st.subheader("ğŸ” æ·±åº¦ç©¿é€æœç´¢")
    search_key = st.text_input("æœç´¢æ–°é—»å…¨æ–‡", placeholder="è¾“å…¥å…³é”®è¯...")
    if search_key:
        search_res = news_df[news_df['content'].str.contains(search_key)]
        st.write(f"å…±å‘ç° {len(search_res)} æ¡å…³è”ä¿¡æ¯ï¼š")
        st.dataframe(search_res[['time', 'title']], use_container_width=True)

else:
    st.error("âŒ æ•°æ®æºè¿æ¥å—é˜»ã€‚è¯·æ£€æŸ¥ AkShare æ˜¯å¦æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œæˆ–åœ¨æœ¬åœ°ç½‘ç»œç¯å¢ƒä¸‹è¿è¡Œã€‚")

# 7.4 å®¡è®¡è„šæ³¨
st.markdown("---")
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.8em;'>
é€»è¾‘ï¼š[RSSæŠ“å–] -> [è¯æƒåŠ åˆ†] -> [è¡Œä¸šæ˜ å°„] -> [è¡Œæƒ…ä¸‹é’»]<br>
Novaï¼Œå½“å‰ç³»ç»Ÿå·²é›†æˆä½ æä¾›çš„æ‰€æœ‰è¡Œä¸šæ˜ å°„ï¼Œå®ç°äº†ä»æ”¿ç­–åˆ°ä¸ªè‚¡çš„ç©¿é€å®¡è®¡ã€‚
</div>
""", unsafe_allow_html=True)
