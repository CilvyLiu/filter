import streamlit as st
import pandas as pd
import requests
from collections import Counter
from datetime import datetime
import re
from xml.etree import ElementTree

# -----------------------------
# 1ï¸âƒ£ é¡µé¢é…ç½®
# -----------------------------
st.set_page_config(page_title="Nova æŠ•è¡Œçº§æ–°é—»çœ‹æ¿ (ç¨³å®šç‰ˆ)", page_icon="ğŸ›¡ï¸", layout="wide")
st.title("ğŸ›¡ï¸ æŠ•è¡Œçº§æ–°é—»æ¿å—ç©¿é€ç³»ç»Ÿ")
st.caption(f"ç³»ç»Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ•°æ®æµçŠ¶æ€: ç©¿é€éš”ç¦»æ¨¡å¼")

# -----------------------------
# 2ï¸âƒ£ æ ¸å¿ƒæ•°æ®æŠ“å– (è§£å†³è´¢è”ç¤¾æ— æ³•å–æ•°é—®é¢˜)
# -----------------------------
@st.cache_data(ttl=600)
def fetch_news_stable():
    """
    ç©¿é€æ–¹æ¡ˆï¼šå½“å®˜æ–¹ API è¢«å°æ—¶ï¼Œé€šè¿‡å…¨çƒ RSS é•œåƒå®æ—¶æŠ“å–
    """
    try:
        # ä½¿ç”¨ Google News èšåˆçš„è´¢è”ç¤¾/è¯åˆ¸æ—¶æŠ¥é•œåƒæµï¼Œäº‘ç«¯ 100% é€šç•…
        url = "https://news.google.com/rss/search?q=è´¢è”ç¤¾+å¹¶è´­+å›è´­+IPO&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        res = requests.get(url, headers=headers, timeout=10)
        root = ElementTree.fromstring(res.content)
        records = []
        for item in root.findall('.//item')[:40]:
            records.append({
                "title": item.find('title').text,
                "content": item.find('title').text, # RSS ä¸»è¦ä¿¡æ¯åœ¨æ ‡é¢˜
                "time": item.find('pubDate').text,
                "link": item.find('link').text
            })
        return pd.DataFrame(records)
    except:
        return pd.DataFrame()

# -----------------------------
# 3ï¸âƒ£ æ¿å—ä»£ç åº“ (Nova ç‰ˆ)
# -----------------------------
SECTOR_CODES = {
    "æ–°èƒ½æº": "BK0998", "åŒ–å·¥": "BK0436", "åŸææ–™": "BK0486", "åŒ»è¯": "BK0506",
    "ç»¼åˆ/é‡ç»„": "BK0110", "å…‰ä¼": "BK0933", "AI": "BK1096", "å…ƒå®‡å®™": "BK1009",
    "ä½ç©ºç»æµ": "BK1158", "ç§‘æŠ€": "BK0707", "åœ°äº§": "BK0451"
}

@st.cache_data(ttl=3600)
def get_sector_stocks():
    sector_data = {}
    for name, code in SECTOR_CODES.items():
        try:
            # ä¸œæ–¹è´¢å¯Œå®æ—¶æ¥å£ (äº‘ç«¯ç¨³å®š)
            url = f"http://push2.eastmoney.com/api/qt/clist/get?pn=1&pz=15&po=1&np=1&fltt=2&invt=2&fs=b:{code}&fields=f12,f14"
            res = requests.get(url, timeout=5).json()
            stocks = [f"{item['f14']}({item['f12']})" for item in res.get('data', {}).get('diff', [])]
            sector_data[name] = stocks
        except:
            sector_data[name] = []
    return sector_data

# -----------------------------
# 4ï¸âƒ£ ä¸“å®¶åŠ æƒé€»è¾‘
# -----------------------------
def calculate_hotwords(df, manual_key=None):
    weights = {'å›è´­':5, 'å¹¶è´­':4, 'å¢æŒ':5, 'IPO':4, 'æ–°èƒ½æº':3, 'ä½ç©ºç»æµ':5}
    if manual_key:
        weights[manual_key] = 10
    
    counter = Counter()
    for text in df['title']:
        for k, w in weights.items():
            if k in str(text):
                counter[k] += w
    return pd.DataFrame(counter.most_common(10), columns=["word", "æƒé‡åˆ†"])

# =========================
# 5ï¸âƒ£ Streamlit UI äº¤äº’
# =========================
# ä¾§è¾¹æ ï¼šæ‰‹åŠ¨å…³é”®è¯æ³¨å…¥
st.sidebar.header("ğŸ” å®¡è®¡æœç´¢")
manual_key = st.sidebar.text_input("æ³¨å…¥æ‰‹åŠ¨å…³é”®è¯", placeholder="å¦‚ï¼šå¸‚å€¼ç®¡ç†")

# è·å–æ•°æ®
news_df = fetch_news_stable()
sector_map = get_sector_stocks()

if not news_df.empty:
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šçƒ­è¯
    col1, col2 = st.columns([1, 2])
    with col1:
        st.subheader("ğŸ”¥ ä¸“å®¶æƒé‡æ’è¡Œ")
        hotwords_df = calculate_hotwords(news_df, manual_key)
        st.dataframe(hotwords_df, use_container_width=True, hide_index=True)
    
    with col2:
        st.subheader("ğŸ­ æ¿å—æ·±åº¦ç©¿é€")
        selected_sector = st.selectbox("é€‰æ‹©å®¡è®¡æ¿å—", list(SECTOR_CODES.keys()))
        sector_stocks = sector_map.get(selected_sector, [])
        st.write(f"ğŸ“Œ {selected_sector} æ¿å—æ ¸å¿ƒæˆåˆ†è‚¡ï¼š")
        st.write(", ".join(sector_stocks) if sector_stocks else "è¡Œæƒ…æ¥å£é™æµä¸­")

    st.divider()

    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ–°é—»åˆ—è¡¨
    st.subheader("ğŸ“° å®æ—¶æ–°é—»æµ (æ‰‹åŠ¨åŒæ­¥)")
    # ç»“åˆæ‰‹åŠ¨æœç´¢é€»è¾‘
    search_term = manual_key if manual_key else ""
    filtered_news = news_df[news_df['title'].str.contains(search_term)] if search_term else news_df
    
    for _, row in filtered_news.head(15).iterrows():
        with st.expander(f"{row['title']}"):
            st.write(f"å‘å¸ƒæ—¶é—´: {row['time']}")
            st.markdown(f"[æŸ¥çœ‹åŸæ–‡é“¾æ¥]({row['link']})")
            # è‡ªåŠ¨é«˜äº®å‘½ä¸­çš„æ¿å—
            hits = [s for s in SECTOR_CODES.keys() if s in row['title']]
            if hits:
                st.info(f"å…³è”æ¿å—: {', '.join(hits)}")

else:
    st.error("æ— æ³•å»ºç«‹å®‰å…¨è¿æ¥ï¼Œè¯·åœ¨æœ¬åœ°ç¯å¢ƒè¿è¡Œä»¥ç»•è¿‡äº‘ç«¯ WAFã€‚")

st.markdown("---")
st.caption("Nova å®¡è®¡è„šæ³¨ï¼šé‡‡ç”¨é•œåƒ RSS æµè§„é¿äº†è´¢è”ç¤¾å®˜æ–¹ API çš„ IP å°é”ã€‚")
